{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Car Racing DQN Agent","text":"![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/SverreNystad/car-racing-dqn-agent/ci.yml) ![GitHub top language](https://img.shields.io/github/languages/top/SverreNystad/car-racing-dqn-agent) ![GitHub language count](https://img.shields.io/github/languages/count/SverreNystad/car-racing-dqn-agent) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Project Version](https://img.shields.io/badge/version-1.0.0-blue)](https://img.shields.io/badge/version-1.0.0-blue)   \ud83d\udccb Table of contents   - [Car Racing DQN Agent](#car-racing-dqn-agent)   - [Description](#description)   - [Features](#features)   - [Results](#results)   - [Prerequisites](#prerequisites)   - [Getting started](#getting-started)   - [Usage](#usage)     - [\ud83d\udcd6 Generate Documentation Site](#-generate-documentation-site)   - [Testing](#testing)     - [License](#license)"},{"location":"#description","title":"Description","text":"<p>This project implements a Deep Q-Network (DQN) agent to play the CarRacing-v3 environment from OpenAI Gym. The agent is trained using reinforcement learning techniques to navigate the racetrack efficiently and managing to achieve max score for the environment, something I can not do myself.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p>DQN Implementation \u2014 A fully functional Deep Q-Learning agent with:</p> </li> <li> <p>Experience replay (<code>TensorDictReplayBuffer</code>)</p> </li> <li>Soft target updates</li> <li>Epsilon-greedy exploration</li> <li>Double DQN target computation</li> <li>Gradient clipping and AdamW optimization</li> <li> <p>Integrated Weights &amp; Biases (W&amp;B) logging for metrics and model artifacts</p> </li> <li> <p>Environment Wrappers \u2014 Preprocessing for Gymnasium environments including:</p> </li> <li> <p>Frame skipping, grayscale conversion, resizing, and frame stacking</p> </li> <li> <p>Automatic video recording and episode statistics tracking</p> </li> <li> <p>Configurable Training \u2014 YAML-based configuration system using Pydantic for environment, training, and logging settings.</p> </li> <li> <p>Human Play \u2014 Play the CarRacing-v3 environment using keyboard controls.</p> </li> </ul>"},{"location":"#results","title":"Results","text":"<p>Here are some videos showing the performance of the trained DQN agent in several environments:  </p> <p>Here is a plot showing the training performance of the DQN agent over time: </p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Git: Ensure that git is installed on your machine. Download Git</li> <li>Python 3.12: Required for the project. Download Python</li> <li>UV: Used for managing Python environments. Install UV</li> <li>WandB: For experiment tracking and visualization. Create an account</li> <li>CUDA: If you plan to use GPU acceleration, ensure that CUDA is installed. Install CUDA</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<ol> <li>Clone the repository:</li> </ol> <p><code>sh    git clone https://github.com/SverreNystad/car-racing-dqn-agent.git    cd car-racing-dqn-agent</code></p> <ol> <li>Install dependencies:</li> </ol> <p><code>sh    uv sync</code></p> <ol> <li>Configure environment variables:    This project uses environment variables for configuration. Copy the example environment file to create your own:</li> </ol> <p><code>sh    cp .env.example .env</code></p> <p>Then edit the <code>.env</code> file to include your specific configuration settings.</p> <ol> <li>Set up pre commit (only for development):</li> </ol> <p><code>sh    uv run pre-commit install</code></p>"},{"location":"#usage","title":"Usage","text":"<p>To run the project, run the following command from the root directory of the project:</p> <p>To train a DQN agent using the default configuration, use:</p> <pre><code>uv run main.py train\n</code></pre> <p>To train a DQN agent with a specific configuration file in <code>configs</code>, use:</p> <pre><code>uv run main.py train --config &lt;name_of_config_file.yaml&gt;\n</code></pre> <p>Then one can change the environment and agent parameters in the config file.</p> <p>To run a saved DQN agent, use:</p> <pre><code>uv run main.py run --agent-name &lt;path_to_model_parameters&gt;\n</code></pre> <p>To play the game using the keyboard, use:</p> <pre><code>uv run main.py play\n</code></pre>"},{"location":"#generate-documentation-site","title":"\ud83d\udcd6 Generate Documentation Site","text":"<p>To build and preview the documentation site locally:</p> <pre><code>uv run mkdocs build\nuv run mkdocs serve\n</code></pre> <p>This will build the documentation and start a local server at http://127.0.0.1:8000/ where you can browse the docs and API reference. Get the documentation according to the lastes commit on main by viewing the <code>gh-pages</code> branch on GitHub: https://sverrenystad.github.io/car-racing-dqn-agent/.</p>"},{"location":"#testing","title":"Testing","text":"<p>To run the test suite, run the following command from the root directory of the project:</p> <pre><code>uv run pytest --doctest-modules --cov=src --cov-report=html\n</code></pre>"},{"location":"#license","title":"License","text":"<p>Distributed under the MIT License. See <code>LICENSE</code> for more information.</p>"},{"location":"reference/src/","title":"src","text":""},{"location":"reference/src/#src","title":"src","text":"<p>Modules:</p> <ul> <li> <code>agent</code>           \u2013            </li> <li> <code>agents</code>           \u2013            </li> <li> <code>configuration</code>           \u2013            </li> <li> <code>environment</code>           \u2013            </li> <li> <code>network</code>           \u2013            <p>Defines the neural network architecture used by the DQN agents.</p> </li> <li> <code>train</code>           \u2013            </li> </ul>"},{"location":"reference/src/agent/","title":"agent","text":""},{"location":"reference/src/agent/#src.agent","title":"src.agent","text":"<p>Classes:</p> <ul> <li> <code>Agent</code>           \u2013            </li> </ul>"},{"location":"reference/src/agent/#src.agent.Agent","title":"Agent","text":"<p>               Bases: <code>Protocol</code></p> <p>Methods:</p> <ul> <li> <code>choose_action</code>             \u2013              <p>Get the action to take based on the current observation.</p> </li> <li> <code>load_policy</code>             \u2013              <p>Load a saved policy into the agent.</p> </li> <li> <code>save_policy</code>             \u2013              <p>Save the current policy.</p> </li> <li> <code>store</code>             \u2013              <p>Store the experience in the agent's replay buffer.</p> </li> <li> <code>update</code>             \u2013              <p>Update the agent's knowledge based on the experiences stored in the replay buffer.</p> </li> </ul>"},{"location":"reference/src/agent/#src.agent.Agent.choose_action","title":"choose_action","text":"<pre><code>choose_action(observation: Union[ndarray, Tensor]) -&gt; int\n</code></pre> <p>Get the action to take based on the current observation.</p> Source code in <code>src/agent.py</code> <pre><code>def choose_action(self, observation: Union[ndarray, Tensor]) -&gt; int:\n    \"\"\"\n    Get the action to take based on the current observation.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/src/agent/#src.agent.Agent.load_policy","title":"load_policy","text":"<pre><code>load_policy(policy_name: str) -&gt; None\n</code></pre> <p>Load a saved policy into the agent.</p> <p>Parameters:</p> Source code in <code>src/agent.py</code> <pre><code>def load_policy(self, policy_name: str) -&gt; None:\n    \"\"\"\n    Load a saved policy into the agent.\n\n    Args:\n        policy_name (str): The name of the policy to load.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/src/agent/#src.agent.Agent.load_policy(policy_name)","title":"<code>policy_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the policy to load.</p>"},{"location":"reference/src/agent/#src.agent.Agent.save_policy","title":"save_policy","text":"<pre><code>save_policy(policy_name: str) -&gt; None\n</code></pre> <p>Save the current policy.</p> <p>Parameters:</p> Source code in <code>src/agent.py</code> <pre><code>def save_policy(self, policy_name: str) -&gt; None:\n    \"\"\"\n    Save the current policy.\n\n    Args:\n        policy_name (str): The name to use when saving the policy.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/src/agent/#src.agent.Agent.save_policy(policy_name)","title":"<code>policy_name</code>","text":"(<code>str</code>)           \u2013            <p>The name to use when saving the policy.</p>"},{"location":"reference/src/agent/#src.agent.Agent.store","title":"store","text":"<pre><code>store(observation: Union[ndarray, Tensor], action: int, reward: float, terminated: bool, next_observation: Union[ndarray, Tensor]) -&gt; None\n</code></pre> <p>Store the experience in the agent's replay buffer.</p> Source code in <code>src/agent.py</code> <pre><code>def store(\n    self,\n    observation: Union[ndarray, Tensor],\n    action: int,\n    reward: float,\n    terminated: bool,\n    next_observation: Union[ndarray, Tensor],\n) -&gt; None:\n    \"\"\"\n    Store the experience in the agent's replay buffer.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/src/agent/#src.agent.Agent.update","title":"update","text":"<pre><code>update() -&gt; dict\n</code></pre> <p>Update the agent's knowledge based on the experiences stored in the replay buffer.</p> <p>Parameters:</p> <p>Returns:     dict: A dictionary containing relevant metrics from the update process (e.g., loss values).</p> Source code in <code>src/agent.py</code> <pre><code>def update(\n    self,\n) -&gt; dict:\n    \"\"\"\n    Update the agent's knowledge based on the experiences stored in the replay buffer.\n\n    Args:\n        batch_size (int): The number of experiences to sample from the replay buffer for each update\n    Returns:\n        dict: A dictionary containing relevant metrics from the update process (e.g., loss values).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/src/agent/#src.agent.Agent.update(batch_size)","title":"<code>batch_size</code>","text":"(<code>int</code>)           \u2013            <p>The number of experiences to sample from the replay buffer for each update</p>"},{"location":"reference/src/agents/DQN_agent/","title":"DQN_agent","text":""},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent","title":"src.agents.DQN_agent","text":"<p>Classes:</p> <ul> <li> <code>DQNAgent</code>           \u2013            </li> </ul>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent","title":"DQNAgent","text":"<pre><code>DQNAgent(state_shape: tuple, action_size: int, epsilon_start: float = 1.0, epsilon_end: float = 0.01, epsilon_decay_steps: int = 40000, learning_rate: float = 0.001, discount_factor: float = 0.99, batch_size: int = 64, tau: float = 0.005)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>Initializes the DQN agent with the given parameters.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>load_policy</code>             \u2013              <p>Loads the policy network's parameters from a file.</p> </li> <li> <code>save_policy</code>             \u2013              <p>Saves the policy network's parameters to WandB artifacts.</p> </li> </ul> Source code in <code>src/agents/DQN_agent.py</code> <pre><code>def __init__(\n    self,\n    state_shape: tuple,\n    action_size: int,\n    epsilon_start: float = 1.0,\n    epsilon_end: float = 0.01,\n    epsilon_decay_steps: int = 40_000,\n    learning_rate: float = 0.001,\n    discount_factor: float = 0.99,\n    batch_size: int = 64,\n    tau: float = 0.005,\n):\n    \"\"\"\n    Initializes the DQN agent with the given parameters.\n\n    Args:\n        state_shape (tuple): The shape of the state/observation space.\n        action_size (int): The number of possible actions.\n        epsilon (float): Initial exploration rate for epsilon-greedy policy.\n        epsilon_decay (float): Decay rate for epsilon after each episode.\n        epsilon_start (float): Starting value of epsilon.\n        epsilon_end (float): Minimum value of epsilon.\n        learning_rate (float): Learning rate for the optimizer.\n        discount_factor (float): Discount factor \"gamma\" for future rewards.\n        batch_size (int): Number of experiences to sample from the replay buffer for each update.\n        tau (float): Soft update parameter for target network updates.\n    \"\"\"\n    self.action_size = action_size\n    self.epsilon_start = epsilon_start\n    self.epsilon_end = epsilon_end\n    self.epsilon = epsilon_start\n    self.epsilon_decay_steps = epsilon_decay_steps\n\n    self.learning_rate = learning_rate\n    self.discount_factor = discount_factor\n    self.batch_size = batch_size\n    self.tau = tau\n\n    # if GPU is to be used\n    self.device = torch.device(\n        \"cuda\"\n        if torch.cuda.is_available()\n        else \"mps\"\n        if torch.backends.mps.is_available()\n        else \"cpu\"\n    )\n\n    self.policy_network = DQN(\n        input_shape=state_shape,\n        action_space_size=action_size,\n    ).to(self.device)\n    # Create the target network and initialize it with the same weights as the main network\n    self.target_model = DQN(\n        input_shape=state_shape,\n        action_space_size=action_size,\n    ).to(self.device)\n    self.target_model.load_state_dict(self.policy_network.state_dict())\n\n    # Track gradients/params (lightweight default)\n    wandb.watch(self.policy_network, log=\"gradients\", log_freq=1000)\n    self.replay_buffer = TensorDictReplayBuffer(\n        storage=LazyMemmapStorage(\n            max_size=50_000,\n        ),\n        # batch_size=[batch_size],\n    )\n\n    self.optimizer = torch.optim.AdamW(\n        self.policy_network.parameters(),\n        lr=self.learning_rate,\n        amsgrad=True,\n    )\n    self.criterion = torch.nn.SmoothL1Loss()\n\n    self.global_step = 0\n</code></pre>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(state_shape)","title":"<code>state_shape</code>","text":"(<code>tuple</code>)           \u2013            <p>The shape of the state/observation space.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(action_size)","title":"<code>action_size</code>","text":"(<code>int</code>)           \u2013            <p>The number of possible actions.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(epsilon)","title":"<code>epsilon</code>","text":"(<code>float</code>)           \u2013            <p>Initial exploration rate for epsilon-greedy policy.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(epsilon_decay)","title":"<code>epsilon_decay</code>","text":"(<code>float</code>)           \u2013            <p>Decay rate for epsilon after each episode.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(epsilon_start)","title":"<code>epsilon_start</code>","text":"(<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Starting value of epsilon.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(epsilon_end)","title":"<code>epsilon_end</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Minimum value of epsilon.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(learning_rate)","title":"<code>learning_rate</code>","text":"(<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning rate for the optimizer.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(discount_factor)","title":"<code>discount_factor</code>","text":"(<code>float</code>, default:                   <code>0.99</code> )           \u2013            <p>Discount factor \"gamma\" for future rewards.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(batch_size)","title":"<code>batch_size</code>","text":"(<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Number of experiences to sample from the replay buffer for each update.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent(tau)","title":"<code>tau</code>","text":"(<code>float</code>, default:                   <code>0.005</code> )           \u2013            <p>Soft update parameter for target network updates.</p>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent.load_policy","title":"load_policy","text":"<pre><code>load_policy(policy_path: str) -&gt; None\n</code></pre> <p>Loads the policy network's parameters from a file.</p> Source code in <code>src/agents/DQN_agent.py</code> <pre><code>def load_policy(self, policy_path: str) -&gt; None:\n    \"\"\"\n    Loads the policy network's parameters from a file.\n    \"\"\"\n    logger.info(f\"Loading model from {policy_path}\")\n    self.policy_network.load_state_dict(\n        torch.load(policy_path, map_location=self.device)\n    )\n    self.target_model.load_state_dict(self.policy_network.state_dict())\n    logger.info(f\"Loaded model from {policy_path}.\")\n    self.epsilon = 0\n</code></pre>"},{"location":"reference/src/agents/DQN_agent/#src.agents.DQN_agent.DQNAgent.save_policy","title":"save_policy","text":"<pre><code>save_policy(policy_name: str) -&gt; None\n</code></pre> <p>Saves the policy network's parameters to WandB artifacts.</p> Source code in <code>src/agents/DQN_agent.py</code> <pre><code>def save_policy(self, policy_name: str) -&gt; None:\n    \"\"\"\n    Saves the policy network's parameters to WandB artifacts.\n    \"\"\"\n\n    # Save your model.\n    FILE_NAME = f\"models/{policy_name}.pth\"\n    torch.save(self.policy_network.state_dict(), FILE_NAME)\n    # Save as artifact for version control.\n    artifact = wandb.Artifact(\"model\", type=\"model\")\n    artifact.add_file(FILE_NAME)\n    wandb.log_artifact(artifact)\n    logger.info(f\"Saved model as {policy_name}.pth and logged to WandB.\")\n</code></pre>"},{"location":"reference/src/agents/","title":"agents","text":""},{"location":"reference/src/agents/#src.agents","title":"src.agents","text":"<p>Modules:</p> <ul> <li> <code>DQN_agent</code>           \u2013            </li> </ul>"},{"location":"reference/src/agents/random_agent/","title":"random_agent","text":""},{"location":"reference/src/agents/random_agent/#src.agents.random_agent","title":"src.agents.random_agent","text":""},{"location":"reference/src/configuration/","title":"configuration","text":""},{"location":"reference/src/configuration/#src.configuration","title":"src.configuration","text":"<p>Classes:</p> <ul> <li> <code>TrainingConfiguration</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>load_config</code>             \u2013              <p>Load a configuration file from the config directory.</p> </li> </ul>"},{"location":"reference/src/configuration/#src.configuration.TrainingConfiguration","title":"TrainingConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> <ul> <li> <code>replay_buffer_size</code>               (<code>int</code>)           \u2013            <p>size of the storage, i.e. maximum number of elements stored in the buffer.</p> </li> </ul>"},{"location":"reference/src/configuration/#src.configuration.TrainingConfiguration.replay_buffer_size","title":"replay_buffer_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>replay_buffer_size: int = 100000\n</code></pre> <p>size of the storage, i.e. maximum number of elements stored in the buffer.</p>"},{"location":"reference/src/configuration/#src.configuration.load_config","title":"load_config","text":"<pre><code>load_config(filename: str) -&gt; Configuration\n</code></pre> <p>Load a configuration file from the config directory.</p> Source code in <code>src/configuration.py</code> <pre><code>def load_config(filename: str) -&gt; \"Configuration\":\n    \"\"\"\n    Load a configuration file from the config directory.\n    \"\"\"\n    path = os.path.join(CONFIG_PATH, filename)\n    with open(path) as file:\n        raw_config = yaml.safe_load(file)\n    return Configuration(**raw_config)\n</code></pre>"},{"location":"reference/src/environment/","title":"environment","text":""},{"location":"reference/src/environment/#src.environment","title":"src.environment","text":"<p>Classes:</p> <ul> <li> <code>ActionFrameRepeater</code>           \u2013            <p>A wrapper for skipping frames in the environment to speed up training.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_env</code>             \u2013              <p>Create and wrap a Gymnasium environment with common wrappers.</p> </li> </ul>"},{"location":"reference/src/environment/#src.environment.ActionFrameRepeater","title":"ActionFrameRepeater","text":"<pre><code>ActionFrameRepeater(env: Env, action_repeat: int = 4)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>A wrapper for skipping frames in the environment to speed up training.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>step</code>             \u2013              <p>Executes the action for the specified number of frames, accumulating rewards.</p> </li> </ul> Source code in <code>src/environment.py</code> <pre><code>def __init__(self, env: gym.Env, action_repeat: int = 4):\n    super().__init__(env)\n    if action_repeat &lt; 1:\n        raise ValueError(\"Action repeat must be at least 1\")\n    self._steps = action_repeat\n</code></pre>"},{"location":"reference/src/environment/#src.environment.ActionFrameRepeater(env (gymnasium.Env) )","title":"<code>env (gymnasium.Env) </code>","text":"\u2013            <p>The environment to apply the wrapper to.</p>"},{"location":"reference/src/environment/#src.environment.ActionFrameRepeater(skip (int) )","title":"<code>skip (int) </code>","text":"\u2013            <p>The number of frames to skip.</p>"},{"location":"reference/src/environment/#src.environment.ActionFrameRepeater.step","title":"step","text":"<pre><code>step(action: int) -&gt; tuple\n</code></pre> <p>Executes the action for the specified number of frames, accumulating rewards.</p> Source code in <code>src/environment.py</code> <pre><code>def step(self, action: int) -&gt; tuple:\n    \"\"\"\n    Executes the action for the specified number of frames, accumulating rewards.\n    \"\"\"\n    total_reward = 0.0\n    for _ in range(self._steps):\n        state, reward, terminated, truncated, info = self.env.step(action)\n        total_reward += float(reward)\n        if terminated or truncated:\n            break\n    return state, total_reward, terminated, truncated, info\n</code></pre>"},{"location":"reference/src/environment/#src.environment.create_env","title":"create_env","text":"<pre><code>create_env(env_id: str, continuous: bool = False, video_folder_name: str | None = None, action_repeat: int = 4, training_record_frequency: int = 250, frame_size: tuple[int, int] = (84, 84)) -&gt; gym.Env\n</code></pre> <p>Create and wrap a Gymnasium environment with common wrappers.</p> <p>The environment is wrapped with: - GrayscaleObservation: Converts RGB observations to grayscale. - ResizeObservation: Resizes observations to the specified frame size. - ActionFrameRepeater: Repeats each action for a specified number of frames. - RecordVideo: Records videos of episodes at a specified frequency. - RecordEpisodeStatistics: Records episode statistics such as total reward and length. - FrameStackObservation: Stacks the last N frames to provide temporal context.</p> <p>Parameters:</p> <p>Returns:     gym.Env: The wrapped Gymnasium environment.</p> Source code in <code>src/environment.py</code> <pre><code>def create_env(\n    env_id: str,\n    continuous: bool = False,\n    video_folder_name: str | None = None,\n    action_repeat: int = 4,\n    training_record_frequency: int = 250,\n    frame_size: tuple[int, int] = (84, 84),\n) -&gt; gym.Env:\n    \"\"\"Create and wrap a Gymnasium environment with common wrappers.\n\n    The environment is wrapped with:\n    - GrayscaleObservation: Converts RGB observations to grayscale.\n    - ResizeObservation: Resizes observations to the specified frame size.\n    - ActionFrameRepeater: Repeats each action for a specified number of frames.\n    - RecordVideo: Records videos of episodes at a specified frequency.\n    - RecordEpisodeStatistics: Records episode statistics such as total reward and length.\n    - FrameStackObservation: Stacks the last N frames to provide temporal context.\n\n    Args:\n        env_id (str): The Gymnasium environment ID.\n        continuous (bool): Whether to create a continuous action space environment.\n        video_folder_name (str | None): Folder to save recorded videos. If None, defaults\n            to \"{env_id}-training\".\n        action_repeat (int): Number of frames to repeat each action.\n        training_record_frequency (int): Frequency (in episodes) to record videos.\n        frame_size (tuple[int, int]): The desired frame size for observations.\n    Returns:\n        gym.Env: The wrapped Gymnasium environment.\n    \"\"\"\n    if any(dim &lt;= 0 for dim in frame_size):\n        raise ValueError(\n            f\"Invalid frame size {frame_size} for environment {env_id} all dimensions must be positive\"\n        )\n\n    # Some environments (for example CartPole) do not accept a `continuous`\n    try:\n        env = gym.make(env_id, render_mode=\"rgb_array\", continuous=continuous)\n    except TypeError as e:\n        logger.warning(f\"Creating env {env_id} without 'continuous' argument: {e}\")\n        env = gym.make(env_id, render_mode=\"rgb_array\")\n\n    # Only apply ResizeObservation for image-like observation spaces.\n    # Some environments (e.g. CartPole) provide a low-dimensional observation (vector) rather than pixel observations.\n    obs_shape = env.observation_space.shape\n    if obs_shape and len(obs_shape) in {2, 3}:\n        env = GrayscaleObservation(env)\n        env = ResizeObservation(env, frame_size)\n        env = FrameStackObservation(env, stack_size=action_repeat)\n\n    env = ActionFrameRepeater(env, action_repeat=action_repeat)\n    if not video_folder_name:\n        video_folder_name = f\"{env_id}-training\"\n    env = RecordVideo(\n        env,\n        video_folder=video_folder_name,\n        name_prefix=\"training\",\n        episode_trigger=lambda x: x % training_record_frequency == 0,\n    )\n    env = RecordEpisodeStatistics(env)\n    # Make sure the trainer can find the videos\n    setattr(env, \"video_folder_name\", video_folder_name)\n    return env\n</code></pre>"},{"location":"reference/src/environment/#src.environment.create_env(env_id)","title":"<code>env_id</code>","text":"(<code>str</code>)           \u2013            <p>The Gymnasium environment ID.</p>"},{"location":"reference/src/environment/#src.environment.create_env(continuous)","title":"<code>continuous</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to create a continuous action space environment.</p>"},{"location":"reference/src/environment/#src.environment.create_env(video_folder_name)","title":"<code>video_folder_name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Folder to save recorded videos. If None, defaults to \"{env_id}-training\".</p>"},{"location":"reference/src/environment/#src.environment.create_env(action_repeat)","title":"<code>action_repeat</code>","text":"(<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of frames to repeat each action.</p>"},{"location":"reference/src/environment/#src.environment.create_env(training_record_frequency)","title":"<code>training_record_frequency</code>","text":"(<code>int</code>, default:                   <code>250</code> )           \u2013            <p>Frequency (in episodes) to record videos.</p>"},{"location":"reference/src/environment/#src.environment.create_env(frame_size)","title":"<code>frame_size</code>","text":"(<code>tuple[int, int]</code>, default:                   <code>(84, 84)</code> )           \u2013            <p>The desired frame size for observations.</p>"},{"location":"reference/src/network/","title":"network","text":""},{"location":"reference/src/network/#src.network","title":"src.network","text":"<p>Defines the neural network architecture used by the DQN agents.</p>"},{"location":"reference/src/train/","title":"train","text":""},{"location":"reference/src/train/#src.train","title":"src.train","text":""}]}